{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexical Analyzer\n",
    "\n",
    "The first step in designing a compiler is to find and create tokens from input.\n",
    "\n",
    "There are two ways to tokenize text/code:\n",
    "\n",
    "- The text is separated character by character, and then we check if the set of characters refers to a specific token or not.\n",
    "- Split sections when a special character appears (like space, comma, etc.).\n",
    "\n",
    "Here we follow the second method using regular expressions..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, List, Tuple\n",
    "\n",
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize, line_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "code = \"\"\"\n",
    "# This is a useless comment\n",
    "print `What is your name? `.\n",
    "my $name <- readLn.\n",
    "printLn `I think you are $name!`.\n",
    "\n",
    "for my $i in 1..10 # Repeat from 1 to 10\n",
    "  printLn `$i`.\n",
    "done\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "code2 = \"\"\"\n",
    "int age = 20;\n",
    "std::cout << age << std::endl;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using built-in methods and natural language processing libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#', 'This', 'is', 'a', 'useless', 'comment', 'print', '`What', 'is', 'your', 'name?', '`.', 'my', '$name', '<-', 'readLn.', 'printLn', '`I', 'think', 'you', 'are', '$name!`.', 'for', 'my', '$i', 'in', '1..10', '#', 'Repeat', 'from', '1', 'to', '10', 'printLn', '`$i`.', 'done']\n"
     ]
    }
   ],
   "source": [
    "print(code.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#', 'This', 'is', 'a', 'useless', 'comment', 'print', '`', 'What', 'is', 'your', 'name', '?', '`', '.', 'my', '$', 'name', '<', '-', 'readLn', '.', 'printLn', '`', 'I', 'think', 'you', 'are', '$', 'name', '!', '`', '.', 'for', 'my', '$', 'i', 'in', '1', '..', '10', '#', 'Repeat', 'from', '1', 'to', '10', 'printLn', '`', '$', 'i', '`', '.', 'done']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#', 'This', 'is', 'a', 'useless', 'comment', 'print', '`', 'What', 'is', 'your', 'name', '?', '`.', 'my', '$', 'name', '<-', 'readLn', '.', 'printLn', '`', 'I', 'think', 'you', 'are', '$', 'name', '!`.', 'for', 'my', '$', 'i', 'in', '1', '..', '10', '#', 'Repeat', 'from', '1', 'to', '10', 'printLn', '`$', 'i', '`.', 'done']\n"
     ]
    }
   ],
   "source": [
    "print(wordpunct_tokenize(code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['# This is a useless comment', 'print `What is your name? `.', 'my $name <- readLn.', 'printLn `I think you are $name!`.', 'for my $i in 1..10 # Repeat from 1 to 10', '  printLn `$i`.', 'done']\n"
     ]
    }
   ],
   "source": [
    "print(line_tokenize(code))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create based on need and creativity maybe!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class Token:\n",
    "    name: str\n",
    "    pattern: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class TokenState:\n",
    "    name: str\n",
    "    value: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Tokens:\n",
    "    dict_: dict = field(init=False, default_factory=dict)\n",
    "\n",
    "    def add(self, name: str, pattern: str) -> None:\n",
    "        \"\"\"Adds the token to the dictionary.\"\"\"\n",
    "        self.dict_[name] = Token(name, pattern)\n",
    "\n",
    "    def add_from_sequence(self, seq: List[Tuple[str, str]]) -> None:\n",
    "        \"\"\"Adds a set of tokens to the dictionary.\"\"\"\n",
    "        for name, pattern in seq:\n",
    "            self.dict_[name] = Token(name, pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Lexer:\n",
    "    def __init__(self, tokens: Tokens, separator: Optional[str] = None) -> None:\n",
    "        self.tokens = tokens\n",
    "        self.separator = separator if separator is not None else r'(\\W)'\n",
    "\n",
    "    def tokenize(self, code: str) -> List[TokenState]:\n",
    "        \"\"\"It is clear from the name of the function what it does.\"\"\"\n",
    "        parts = filter(lambda x: x, re.split(self.separator, code))\n",
    "        result = []\n",
    "\n",
    "        for part in parts:\n",
    "            for token in self.tokens.dict_.values():\n",
    "                match = re.fullmatch(token.pattern, part)\n",
    "                if match is not None:\n",
    "                    result.append(TokenState(token.name, match.group()))\n",
    "                    break\n",
    "            else:\n",
    "                raise ValueError(f'{repr(part)} is not a valid token!')\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our anonymous programming language tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'COMMENT': Token(name='COMMENT', pattern='#.*'),\n",
       " 'DOT': Token(name='DOT', pattern='\\\\.'),\n",
       " 'BETWEEN': Token(name='BETWEEN', pattern='\\\\.{2}'),\n",
       " 'ARROW': Token(name='ARROW', pattern='\\\\<\\\\-'),\n",
       " 'MY': Token(name='MY', pattern='my'),\n",
       " 'FOR': Token(name='FOR', pattern='for'),\n",
       " 'IN': Token(name='IN', pattern='in'),\n",
       " 'DONE': Token(name='DONE', pattern='done'),\n",
       " 'IF': Token(name='IF', pattern='if'),\n",
       " 'DO': Token(name='DO', pattern='do'),\n",
       " 'ELSEIF': Token(name='ELSEIF', pattern='elseif'),\n",
       " 'END': Token(name='END', pattern='end'),\n",
       " 'NUMBER': Token(name='NUMBER', pattern='[\\\\+\\\\-]?\\\\d+(\\\\.\\\\d+)?'),\n",
       " 'STRING': Token(name='STRING', pattern='`.*`'),\n",
       " 'IDENTIFIER': Token(name='IDENTIFIER', pattern='\\\\$?[A-Za-z]+[_A-Za-z0-9]*')}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = Tokens()\n",
    "tokens.add_from_sequence([\n",
    "    ('COMMENT', r'#.*'),\n",
    "    ('DOT', r'\\.'),\n",
    "    ('BETWEEN', r'\\.{2}'),\n",
    "    ('ARROW', r'\\<\\-'),\n",
    "    ('MY', r'my'),\n",
    "    ('FOR', r'for'),\n",
    "    ('IN', r'in'),\n",
    "    ('DONE', r'done'),\n",
    "    ('IF', r'if'),\n",
    "    ('DO', r'do'),\n",
    "    ('ELSEIF', r'elseif'),\n",
    "    ('END', r'end'),\n",
    "    ('NUMBER', r'[\\+\\-]?\\d+(\\.\\d+)?'),\n",
    "    ('STRING', r'`.*`'),\n",
    "    ('IDENTIFIER', r'\\$?[A-Za-z]+[_A-Za-z0-9]*'),\n",
    "])\n",
    "tokens.dict_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making the separator pattern a little complex!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _token_or_pattern(separator: str) -> None:\n",
    "    \"\"\"Returns the separator itself if a token not found.\"\"\"\n",
    "    token = tokens.dict_.get(separator)\n",
    "    return f'({token.pattern})' if token is not None else separator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(`.*`)|(#.*)|(\\\\$?[A-Za-z]+[_A-Za-z0-9]*)|(\\\\.{2})|(\\\\<\\\\-)|\\\\s|(\\\\W)'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "separators = [\n",
    "    'STRING', 'COMMENT', 'IDENTIFIER', 'BETWEEN',\n",
    "    'ARROW', r'\\s', r'(\\W)']  # The order does matter\n",
    "separator = '|'.join(map(_token_or_pattern, separators))\n",
    "separator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TokenState(name='COMMENT', value='# This is a useless comment'),\n",
       " TokenState(name='IDENTIFIER', value='print'),\n",
       " TokenState(name='STRING', value='`What is your name? `'),\n",
       " TokenState(name='DOT', value='.'),\n",
       " TokenState(name='MY', value='my'),\n",
       " TokenState(name='IDENTIFIER', value='$name'),\n",
       " TokenState(name='ARROW', value='<-'),\n",
       " TokenState(name='IDENTIFIER', value='readLn'),\n",
       " TokenState(name='DOT', value='.'),\n",
       " TokenState(name='IDENTIFIER', value='printLn'),\n",
       " TokenState(name='STRING', value='`I think you are $name!`'),\n",
       " TokenState(name='DOT', value='.'),\n",
       " TokenState(name='FOR', value='for'),\n",
       " TokenState(name='MY', value='my'),\n",
       " TokenState(name='IDENTIFIER', value='$i'),\n",
       " TokenState(name='IN', value='in'),\n",
       " TokenState(name='NUMBER', value='1'),\n",
       " TokenState(name='BETWEEN', value='..'),\n",
       " TokenState(name='NUMBER', value='10'),\n",
       " TokenState(name='COMMENT', value='# Repeat from 1 to 10'),\n",
       " TokenState(name='IDENTIFIER', value='printLn'),\n",
       " TokenState(name='STRING', value='`$i`'),\n",
       " TokenState(name='DOT', value='.'),\n",
       " TokenState(name='DONE', value='done')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexer = Lexer(tokens, separator)\n",
    "lexer.tokenize(code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test on a C++ code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'=' is not a valid token!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mlexer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode2\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[17], line 18\u001b[0m, in \u001b[0;36mLexer.tokenize\u001b[0;34m(self, code)\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 18\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(part)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a valid token!\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mValueError\u001b[0m: '=' is not a valid token!"
     ]
    }
   ],
   "source": [
    "lexer.tokenize(code2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
