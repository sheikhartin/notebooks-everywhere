{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexical Analyzer\n",
    "\n",
    "The first step in designing a compiler is to find and create tokens from input.\n",
    "\n",
    "There are two ways to tokenize text/code:\n",
    "\n",
    "- The text is separated character by character, and then we check whether the set of characters in the sequence also refers to a specific sign.\n",
    "- Split sections when a special character appears (like space, comma, etc.).\n",
    "\n",
    "Here we follow the second method using regular expressions..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from functools import partial\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, Callable, Type, List\n",
    "\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "code = \"\"\"\n",
    "// This is a useless comment\n",
    "print \"What is your name? \".\n",
    "my $name <- readLn.\n",
    "printLn \"I think you are ${name}!\".\n",
    "\n",
    "for my $i in 1..10 // Repeat from 1 to 10\n",
    "  printLn \"${i}\".\n",
    "done\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "code2 = \"\"\"\n",
    "int age = 20;\n",
    "std::cout << age << std::endl;\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using built-in methods and natural language processing libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['//', 'This', 'is', 'a', 'useless', 'comment', 'print', '\"What', 'is', 'your', 'name?', '\".', 'my', '$name', '<-', 'readLn.', 'printLn', '\"I', 'think', 'you', 'are', '${name}!\".', 'for', 'my', '$i', 'in', '1..10', '//', 'Repeat', 'from', '1', 'to', '10', 'printLn', '\"${i}\".', 'done']\n"
     ]
    }
   ],
   "source": [
    "split_by_space_method = code.split()\n",
    "print(split_by_space_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['//', 'This', 'is', 'a', 'useless', 'comment', 'print', '``', 'What', 'is', 'your', 'name', '?', '``', '.', 'my', '$', 'name', '<', '-', 'readLn', '.', 'printLn', '``', 'I', 'think', 'you', 'are', '$', '{', 'name', '}', '!', \"''\", '.', 'for', 'my', '$', 'i', 'in', '1', '..', '10', '//', 'Repeat', 'from', '1', 'to', '10', 'printLn', '``', '$', '{', 'i', '}', \"''\", '.', 'done']\n"
     ]
    }
   ],
   "source": [
    "nltk_word_tokenize_method = word_tokenize(code)\n",
    "print(nltk_word_tokenize_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name?', '\"I', '1..10', '${name}!\".', '$i', '$name', '\".', '\"${i}\".', 'readLn.', '\"What', '<-'}\n"
     ]
    }
   ],
   "source": [
    "difference = set(split_by_space_method).difference(set(nltk_word_tokenize_method))\n",
    "print(difference)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create based on need and creativity maybe!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class Token:\n",
    "    name: str\n",
    "    pattern: str\n",
    "    calls: Optional[Callable] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class TokenState:\n",
    "    name: str\n",
    "    value: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Tokens:\n",
    "    dict_: dict = field(init=False, default_factory=dict)\n",
    "    \n",
    "    def add(self, name: str, pattern: str, calls: Optional[str] = None) -> None:\n",
    "        \"\"\"Adds the token as a class attribute and a dictionary item.\"\"\"\n",
    "        setattr(self, name, Token(name, pattern, calls))\n",
    "        self.dict_[name] = Token(name, pattern, calls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Lexer:\n",
    "    tokens: Type[Tokens]\n",
    "    separator: Optional[str] = r'(\\w+)|\\s'\n",
    "    \n",
    "    def tokenize(self, code: str) -> List[Type[TokenState]]:\n",
    "        \"\"\"It is clear from the name of the function what it does.\"\"\"\n",
    "        result = []\n",
    "        parts = filter(lambda x: x, re.split(f'{self.separator}', code))\n",
    "\n",
    "        for part in parts:\n",
    "            for token_name, token_info in self.tokens.dict_.items():\n",
    "                match = re.match(f'{token_info.pattern}$', part)\n",
    "                if match is not None:\n",
    "                    result.append(TokenState(token_name, match.group()))\n",
    "                    break\n",
    "            else:\n",
    "                raise ValueError(f'{repr(part)} is not a valid token!')\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our anonymous programming language tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = Tokens()\n",
    "tokens.add('COMMENT', r'/{2}.*')\n",
    "tokens.add('DOT', r'\\.')\n",
    "tokens.add('BETWEEN', r'\\.{2}')\n",
    "tokens.add('DOLLARSIGN', r'\\$')\n",
    "tokens.add('ARROW', r'<\\-')\n",
    "tokens.add('NUMBER', r'[\\+\\-]?(\\d+|\\d+\\.\\d*|\\d*\\.\\d+)')\n",
    "tokens.add('STRING', r'\".*\"')\n",
    "tokens.add('PRINT', r'print', partial(print, end=''))\n",
    "tokens.add('PRINTLINE', r'printLn', print)\n",
    "tokens.add('READLINE', r'readLn', input)\n",
    "tokens.add('MY', r'my')\n",
    "tokens.add('IN', r'in')\n",
    "tokens.add('IDENTIFIER', r'\\$[A-Za-z]+[_A-Za-z0-9]*')\n",
    "tokens.add('FOR', r'for')\n",
    "tokens.add('DONE', r'done')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making the separator pattern a little complex!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\".*\")|(\\.{2})|([\\+\\-]?(\\d+|\\d+\\.\\d*|\\d*\\.\\d+))|(/{2}.*)|(\\.)|(\\(\\)\\{\\}\\[\\])|\\s\n"
     ]
    }
   ],
   "source": [
    "separator_tokens = ['STRING', 'BETWEEN', 'NUMBER', 'COMMENT', 'DOT']\n",
    "separator_pattern = '|'.join([\n",
    "    f'({getattr(tokens, token_name).pattern})'\n",
    "    for token_name in separator_tokens\n",
    "])\n",
    "separator_pattern += '|(\\(\\)\\{\\}\\[\\])|\\s'\n",
    "print(separator_pattern)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*NOTE: The order in the pattern above is important.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TokenState(name='COMMENT', value='// This is a useless comment'), TokenState(name='PRINT', value='print'), TokenState(name='STRING', value='\"What is your name? \"'), TokenState(name='DOT', value='.'), TokenState(name='MY', value='my'), TokenState(name='IDENTIFIER', value='$name'), TokenState(name='ARROW', value='<-'), TokenState(name='READLINE', value='readLn'), TokenState(name='DOT', value='.'), TokenState(name='PRINTLINE', value='printLn'), TokenState(name='STRING', value='\"I think you are ${name}!\"'), TokenState(name='DOT', value='.'), TokenState(name='FOR', value='for'), TokenState(name='MY', value='my'), TokenState(name='IDENTIFIER', value='$i'), TokenState(name='IN', value='in'), TokenState(name='NUMBER', value='1'), TokenState(name='NUMBER', value='1'), TokenState(name='BETWEEN', value='..'), TokenState(name='NUMBER', value='10'), TokenState(name='NUMBER', value='10'), TokenState(name='COMMENT', value='// Repeat from 1 to 10'), TokenState(name='PRINTLINE', value='printLn'), TokenState(name='STRING', value='\"${i}\"'), TokenState(name='DOT', value='.'), TokenState(name='DONE', value='done')]\n"
     ]
    }
   ],
   "source": [
    "lexer = Lexer(tokens, separator_pattern)\n",
    "print(lexer.tokenize(code))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test on a C++ code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'int' is not a valid token!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m lexer \u001b[39m=\u001b[39m Lexer(tokens, separator_pattern)\n\u001b[0;32m----> 2\u001b[0m \u001b[39mprint\u001b[39m(lexer\u001b[39m.\u001b[39;49mtokenize(code2))\n",
      "Cell \u001b[0;32mIn[15], line 18\u001b[0m, in \u001b[0;36mLexer.tokenize\u001b[0;34m(self, code)\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 18\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mrepr\u001b[39m(part)\u001b[39m}\u001b[39;00m\u001b[39m is not a valid token!\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     20\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "\u001b[0;31mValueError\u001b[0m: 'int' is not a valid token!"
     ]
    }
   ],
   "source": [
    "lexer = Lexer(tokens, separator_pattern)\n",
    "print(lexer.tokenize(code2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
