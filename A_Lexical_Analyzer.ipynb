{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexical Analyzer\n",
    "\n",
    "The first step in designing a compiler is to find and create tokens from input.\n",
    "\n",
    "There are two ways to tokenize text/code:\n",
    "\n",
    "- The text is separated character by character, and then we check if the set of characters refers to a specific token or not.\n",
    "- Split sections when a special character appears (like space, comma, etc.).\n",
    "\n",
    "Here we follow the second method using regular expressions..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from functools import partial\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, Callable, Type, List, Tuple\n",
    "\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "code = \"\"\"\n",
    "# This is a useless comment\n",
    "print `What is your name? `.\n",
    "my $name <- readLn.\n",
    "printLn `I think you are $name!`.\n",
    "\n",
    "for my $i in 1..10 # Repeat from 1 to 10\n",
    "  printLn `$i`.\n",
    "done\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "code2 = \"\"\"\n",
    "int age = 20;\n",
    "std::cout << age << std::endl;\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using built-in methods and natural language processing libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#', 'This', 'is', 'a', 'useless', 'comment', 'print', '`What', 'is', 'your', 'name?', '`.', 'my', '$name', '<-', 'readLn.', 'printLn', '`I', 'think', 'you', 'are', '$name!`.', 'for', 'my', '$i', 'in', '1..10', '#', 'Repeat', 'from', '1', 'to', '10', 'printLn', '`$i`.', 'done']\n"
     ]
    }
   ],
   "source": [
    "print(code.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#', 'This', 'is', 'a', 'useless', 'comment', 'print', '`', 'What', 'is', 'your', 'name', '?', '`', '.', 'my', '$', 'name', '<', '-', 'readLn', '.', 'printLn', '`', 'I', 'think', 'you', 'are', '$', 'name', '!', '`', '.', 'for', 'my', '$', 'i', 'in', '1', '..', '10', '#', 'Repeat', 'from', '1', 'to', '10', 'printLn', '`', '$', 'i', '`', '.', 'done']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(code))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create based on need and creativity maybe!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class Token:\n",
    "    name: str\n",
    "    pattern: str\n",
    "    calls: Optional[Callable] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TokenState:\n",
    "    name: str\n",
    "    value: str\n",
    "    calls: Optional[Callable] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Tokens:\n",
    "    dict_: dict = field(init=False, default_factory=dict)\n",
    "    \n",
    "    def add(self, name: str, pattern: str, calls: Optional[Callable] = None) -> None:\n",
    "        \"\"\"Adds the token to the dictionary.\"\"\"\n",
    "        self.dict_[name] = Token(name, pattern, calls)\n",
    "    \n",
    "    def add_from_sequence(self, seq: List[Tuple[str, str, Optional[Callable]]]) -> None:\n",
    "        \"\"\"Adds a set of tokens to the dictionary.\"\"\"\n",
    "        for name, pattern, *calls in seq:\n",
    "            self.dict_[name] = Token(name, pattern, calls[0] if calls else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Lexer:\n",
    "    tokens: Type[Tokens]\n",
    "    separator: Optional[str] = r'(\\W)'\n",
    "    \n",
    "    def tokenize(self, code: str) -> List[Type[TokenState]]:\n",
    "        \"\"\"It is clear from the name of the function what it does.\"\"\"\n",
    "        parts = filter(lambda x: x, re.split(f'{self.separator}', code))\n",
    "        result = []\n",
    "\n",
    "        for part in parts:\n",
    "            for token in self.tokens.dict_.values():\n",
    "                match = re.match(f'{token.pattern}$', part)\n",
    "                if match is not None:\n",
    "                    result.append(TokenState(token.name, match.group(), token.calls))\n",
    "                    break\n",
    "            else:\n",
    "                raise ValueError(f'{repr(part)} is not a valid token!')\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our anonymous programming language tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'COMMENT': Token(name='COMMENT', pattern='#.*', calls=None),\n",
       " 'DOT': Token(name='DOT', pattern='\\\\.', calls=None),\n",
       " 'BETWEEN': Token(name='BETWEEN', pattern='\\\\.{2}', calls=None),\n",
       " 'ARROW': Token(name='ARROW', pattern='\\\\<\\\\-', calls=None),\n",
       " 'NUMBER': Token(name='NUMBER', pattern='[\\\\+\\\\-]?\\\\d+(\\\\.\\\\d+)?', calls=None),\n",
       " 'STRING': Token(name='STRING', pattern='`.*`', calls=None),\n",
       " 'PRINT': Token(name='PRINT', pattern='print', calls=functools.partial(<built-in function print>, end='')),\n",
       " 'PRINTLINE': Token(name='PRINTLINE', pattern='printLn', calls=<built-in function print>),\n",
       " 'READLINE': Token(name='READLINE', pattern='readLn', calls=<bound method Kernel.raw_input of <ipykernel.ipkernel.IPythonKernel object at 0x7f9f4a51e170>>),\n",
       " 'MY': Token(name='MY', pattern='my', calls=None),\n",
       " 'IN': Token(name='IN', pattern='in', calls=None),\n",
       " 'IDENTIFIER': Token(name='IDENTIFIER', pattern='\\\\$[A-Za-z]+[_A-Za-z0-9]*', calls=None),\n",
       " 'FOR': Token(name='FOR', pattern='for', calls=None),\n",
       " 'DONE': Token(name='DONE', pattern='done', calls=None)}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = Tokens()\n",
    "tokens.add_from_sequence([\n",
    "    ('COMMENT', r'#.*'),\n",
    "    ('DOT', r'\\.'),\n",
    "    ('BETWEEN', r'\\.{2}'),\n",
    "    ('ARROW', r'\\<\\-'),\n",
    "    ('NUMBER', r'[\\+\\-]?\\d+(\\.\\d+)?'),\n",
    "    ('STRING', r'`.*`'),\n",
    "    ('PRINT', r'print', partial(print, end='')),\n",
    "    ('PRINTLINE', r'printLn', print),\n",
    "    ('READLINE', r'readLn', input),\n",
    "    ('MY', r'my'),\n",
    "    ('IN', r'in'),\n",
    "    ('IDENTIFIER', r'\\$[A-Za-z]+[_A-Za-z0-9]*'),\n",
    "    ('FOR', r'for'),\n",
    "    ('DONE', r'done'),\n",
    "])\n",
    "tokens.dict_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making the separator pattern a little complex!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _token_or_pattern(separator: str) -> None:\n",
    "    \"\"\"Returns the separator itself if a token not found.\"\"\"\n",
    "    token = tokens.dict_.get(separator)\n",
    "    return f'({token.pattern})' if token is not None else separator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(`.*`)|(#.*)|(\\\\$[A-Za-z]+[_A-Za-z0-9]*)|(\\\\.{2})|(\\\\<\\\\-)|\\\\s|(\\\\W)'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "separators = [\n",
    "    'STRING', 'COMMENT', 'IDENTIFIER', 'BETWEEN',\n",
    "    'ARROW', '\\s', '(\\W)']  # The order does matter\n",
    "separator = '|'.join(map(_token_or_pattern, separators))\n",
    "separator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TokenState(name='COMMENT', value='# This is a useless comment', calls=None),\n",
       " TokenState(name='PRINT', value='print', calls=functools.partial(<built-in function print>, end='')),\n",
       " TokenState(name='STRING', value='`What is your name? `', calls=None),\n",
       " TokenState(name='DOT', value='.', calls=None),\n",
       " TokenState(name='MY', value='my', calls=None),\n",
       " TokenState(name='IDENTIFIER', value='$name', calls=None),\n",
       " TokenState(name='ARROW', value='<-', calls=None),\n",
       " TokenState(name='READLINE', value='readLn', calls=<bound method Kernel.raw_input of <ipykernel.ipkernel.IPythonKernel object at 0x7f9f4a51e170>>),\n",
       " TokenState(name='DOT', value='.', calls=None),\n",
       " TokenState(name='PRINTLINE', value='printLn', calls=<built-in function print>),\n",
       " TokenState(name='STRING', value='`I think you are $name!`', calls=None),\n",
       " TokenState(name='DOT', value='.', calls=None),\n",
       " TokenState(name='FOR', value='for', calls=None),\n",
       " TokenState(name='MY', value='my', calls=None),\n",
       " TokenState(name='IDENTIFIER', value='$i', calls=None),\n",
       " TokenState(name='IN', value='in', calls=None),\n",
       " TokenState(name='NUMBER', value='1', calls=None),\n",
       " TokenState(name='BETWEEN', value='..', calls=None),\n",
       " TokenState(name='NUMBER', value='10', calls=None),\n",
       " TokenState(name='COMMENT', value='# Repeat from 1 to 10', calls=None),\n",
       " TokenState(name='PRINTLINE', value='printLn', calls=<built-in function print>),\n",
       " TokenState(name='STRING', value='`$i`', calls=None),\n",
       " TokenState(name='DOT', value='.', calls=None),\n",
       " TokenState(name='DONE', value='done', calls=None)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexer = Lexer(tokens, separator)\n",
    "lexer.tokenize(code)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test on a C++ code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'int' is not a valid token!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m lexer\u001b[39m.\u001b[39;49mtokenize(code2)\n",
      "Cell \u001b[0;32mIn[11], line 18\u001b[0m, in \u001b[0;36mLexer.tokenize\u001b[0;34m(self, code)\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 18\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mrepr\u001b[39m(part)\u001b[39m}\u001b[39;00m\u001b[39m is not a valid token!\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     20\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "\u001b[0;31mValueError\u001b[0m: 'int' is not a valid token!"
     ]
    }
   ],
   "source": [
    "lexer.tokenize(code2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
